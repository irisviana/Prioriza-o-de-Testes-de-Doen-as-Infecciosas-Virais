{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import precision_score\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.datasets import make_classification\nimport xgboost as xgb\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nimport matplotlib.pyplot as plt\nimport pickle\nfrom sklearn.metrics import brier_score_loss\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom pprint import pprint\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier \nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import precision_recall_curve\nfrom six import StringIO  \nimport pydot\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data_rapid!","metadata":{}},{"cell_type":"markdown","source":"balanced","metadata":{}},{"cell_type":"code","source":"data_rapid= pd.read_csv(\"../input/dataset-influenza/df_ant_test.csv\")\ndata_rapid=data_rapid.astype(int)\ndata_rapid = data_rapid.replace(0,'n').replace(1,'p').replace('n',1).replace('p',0)\ndel data_rapid[data_rapid.columns[0]]\n\ndata_rapid_rem=data_rapid.copy()\ndel data_rapid_rem['VOMITO']\ndel data_rapid_rem['FADIGA']\ndel data_rapid_rem['PERD_OLFT']\ndel data_rapid_rem['PERD_PALA']\n","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"unbalanced","metadata":{}},{"cell_type":"code","source":"data_rapid_unb= pd.read_csv(\"../input/datasetinfluenzaunbalanced/df_ant_unb.csv\")\ndata_rapid_unb=data_rapid_unb.astype(int)\ndata_rapid_unb = data_rapid_unb.replace(0,'n').replace(1,'p').replace('n',1).replace('p',0)\ndel data_rapid_unb[data_rapid_unb.columns[0]]\n\ndata_rapid_unb_rem=data_rapid_unb.copy()\ndel data_rapid_unb_rem['VOMITO']\ndel data_rapid_unb_rem['FADIGA']\ndel data_rapid_unb_rem['PERD_OLFT']\ndel data_rapid_unb_rem['PERD_PALA']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data_pcr!","metadata":{}},{"cell_type":"code","source":"data_pcr= pd.read_csv(\"../input/dataset-influenza/df_pcr_test.csv\")\ndata_pcr=data_pcr.astype(int)\ndata_pcr=data_pcr.replace(0,'n').replace(1,'p').replace('n',1).replace('p',0)\ndel data_pcr[data_pcr.columns[0]]\n# #removing headache\ndata_pcr_rem=data_pcr.copy()\ndel data_pcr_rem['VOMITO']\ndel data_pcr_rem['FADIGA']\ndel data_pcr_rem['PERD_OLFT']\ndel data_pcr_rem['PERD_PALA']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_pcr_unb= pd.read_csv(\"../input/datasetinfluenzaunbalanced/df_pcr_unb.csv\")\ndata_pcr_unb=data_pcr_unb.astype(int)\ndata_pcr_unb=data_pcr_unb.replace(0,'n').replace(1,'p').replace('n',1).replace('p',0)\ndel data_pcr_unb[data_pcr_unb.columns[0]]\n# #removing headache\ndata_pcr_unb_rem=data_pcr_unb.copy()\ndel data_pcr_unb_rem['VOMITO']\ndel data_pcr_unb_rem['FADIGA']\ndel data_pcr_unb_rem['PERD_OLFT']\ndel data_pcr_unb_rem['PERD_PALA']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading both data!","metadata":{}},{"cell_type":"code","source":"data_both= pd.read_csv(\"../input/dataset-influenza/df_both_test.csv\")\ndata_both=data_both.astype(int)\ndata_both=data_both.replace(0,'n').replace(1,'p').replace('n',1).replace('p',0)\ndel data_both[data_both.columns[0]]\n# #removing headache\ndata_both_rem=data_both.copy()\ndel data_both_rem['VOMITO']\ndel data_both_rem['FADIGA']\ndel data_both_rem['PERD_OLFT']\ndel data_both_rem['PERD_PALA']\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_both_unb= pd.read_csv(\"../input/datasetinfluenzaunbalanced/df_both_unb.csv\")\ndata_both_unb=data_both_unb.astype(int)\ndata_both_unb = data_both_unb.replace(0,'n').replace(1,'p').replace('n',1).replace('p',0)\ndel data_both_unb[data_both_unb.columns[0]]\n# #removing headache\ndata_both_unb_rem=data_both_unb.copy()\ndel data_both_unb_rem['VOMITO']\ndel data_both_unb_rem['FADIGA']\ndel data_both_unb_rem['PERD_OLFT']\ndel data_both_unb_rem['PERD_PALA']\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def RandomForest_classif(x_train,y_train,param):\n    #Classification\n    \n    \n    clf= RandomForestClassifier(**param)\n    clf=clf.fit(x_train, y_train)\n    filename = 'randomForest_model.sav'\n    pickle.dump(clf, open(filename, 'wb'))\n    #filename.to_csv('randomForest_model.sav',index=False)\n    return clf\n\ndef Kneighbors_classif(x_train,y_train,param):\n    #Classification\n    \n\n    clf= KNeighborsClassifier(**param)\n    clf= clf.fit(x_train, y_train)\n    filename = 'kneighbors_model.sav'\n    pickle.dump(clf, open(filename, 'wb'))\n    #filename.to_csv('kneighbors_model.sav',index=False)\n    return clf\n\ndef DecisionTree_classif(x_train,y_train,param):\n    #Classification\n    \n    clf = tree.DecisionTreeClassifier(**param)\n    clf = clf.fit(x_train,y_train)\n    filename = 'decisionTree_model.sav'\n    pickle.dump(clf, open(filename, 'wb'))\n    #filename.to_csv('decisionTree_model.sav',index=False)\n    return clf\n\ndef mpl_classif(x_train,y_train,param):\n    \n    clf =  MLPClassifier(**param)\n    clf=clf.fit(x_train,y_train)\n    filename = 'mlp_model.sav'\n    pickle.dump(clf, open(filename, 'wb'))\n    #filename.to_csv('mlp_model.sav',index=False)\n    return clf\n\ndef gb_classif(x_train,y_train,param):\n    \n    clf=GradientBoostingClassifier(**param)\n    clf=clf.fit(x_train, y_train)\n    filename = 'gradientBoosting_model.sav'\n    pickle.dump(clf, open(filename, 'wb'))\n    #filename.to_csv('gradientBoosting_model.sav',index=False)\n    return clf\n\ndef xgb_classif(x_train,y_train,param):\n    clf = xgb.XGBClassifier(**param)\n    filename = 'xgb_model.sav'\n    pickle.dump(clf, open(filename, 'wb'))\n    #filename.to_csv('xbg_model.sav',index=False)\n    return clf.fit(x_train, y_train)\n\ndef svc_classif(x_train,y_train,param):\n    \n    regr = svm.SVC(**param)\n    regr=regr.fit(x_train, y_train)\n    filename = 'svc_model.sav'\n    pickle.dump(regr, open(filename, 'wb'))\n    #filename.to_csv('svc_model.sav',index=False)\n    return regr\n\ndef lg_with_regu_classif(x_train,y_train,param):\n    \n    clf = LogisticRegression(** param)\n    clf=clf.fit(x_train,y_train)\n    filename = 'lg_regu_model.sav'\n    pickle.dump(clf, open(filename, 'wb'))\n    #filename.to_csv('lg_regu_model.sav',index=False)\n    return clf\n\ndef lg_without_regu_classif(x_train,y_train,param):\n    \n    clf = LogisticRegression(** param)\n    clf=clf.fit(x_train,y_train)\n    filename = 'lg_model.sav'  \n    pickle.dump(clf, open(filename, 'wb'))\n    #filename.to_csv('lg_model.sav',index=False)\n    return clf\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_metrics(x_train, x_test,y_train, y_test,data,k,clf,lista,p,r):\n        \n        \n        #prediction\n        y_pred=clf.predict(x_test)\n    \n        #accuracy score\n        acc=accuracy_score(y_test,y_pred)*100\n        \n        #confusion matrix\n        tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\n        data.iloc[k:k+1,6:7]=tn\n        data.iloc[k:k+1,7:8]=fp\n        data.iloc[k:k+1,8:9]=fn\n        data.iloc[k:k+1,9:10]=tp\n        #ROC CURVE\n        mean_fpr = np.linspace(0, 1, 100)\n        y_prob=clf.predict_proba(x_test)\n        y_prob=y_prob[:, 0]\n        \n        fpr, tpr, thresholds = metrics.roc_curve(y_test,y_prob,pos_label=0)\n        \n        interp_tpr = np.interp(mean_fpr,fpr, tpr)\n        interp_tpr[0] = 0.0\n        lista.append(interp_tpr)\n        \n        #precision-recall curve\n        precision, recall, thresholds = precision_recall_curve( y_test, y_prob)\n        p.append(y_test)\n        r.append(y_prob)\n        #precision\n        data.iloc[k:k+1,:1]=(tp/(tp+fp))*100\n        \n        data.iloc[k:k+1,1:2]=acc\n        \n        #recall\n        data.iloc[k:k+1,2:3]=(tp/(tp+fn))*100\n        \n        \n        #mean absolute error\n        data.iloc[k:k+1,3:4]=mean_absolute_error(y_test, y_pred)*100\n        \n        #AUC\n        data.iloc[k:k+1,4:5]= metrics.roc_auc_score(y_test,y_pred)*100\n        #Brier score\n        data.iloc[k:k+1,5:6]=brier_score_loss(y_test,y_pred)\n        \n        \n        \n        return data,lista,p,r\n    \ndef calculate_metrics_voting(x_train, x_test,y_train, y_test,data,k,clf):\n        \n        \n        #prediction\n        y_pred=clf.predict(x_test)\n        \n        #accuracy score\n        acc=accuracy_score(y_test,y_pred)*100\n        \n        #confusion matrix\n        tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\n        \n        \n        \n        #precision\n        data.iloc[k:k+1,:1]=(tp/(tp+fp))*100\n        \n        data.iloc[k:k+1,1:2]=acc\n        \n        #recall\n        data.iloc[k:k+1,2:3]=(tp/(tp+fn))*100\n        \n        \n        #mean absolute error\n        data.iloc[k:k+1,3:4]=mean_absolute_error(y_test, y_pred)*100\n        \n        #AUC\n        data.iloc[k:k+1,4:5]= metrics.roc_auc_score(y_test,y_pred)*100\n        #Brier score\n        data.iloc[k:k+1,5:6]=brier_score_loss(y_test,y_pred)\n        \n        \n        \n        return data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def k_fold_cross_validation(x,y,lista_param):\n    lista1, lista2, lista3, lista4,lista5,lista6,lista7,lista8,lista9 = ([] for i in range(9))\n    p1, p2, p3, p4,p5,p6,p7,p8,p9 = ([] for i in range(9))\n    r1, r2, r3, r4,r5,r6,r7,r8,r9 = ([] for i in range(9))\n    dt=pd.DataFrame(columns=['P','ACC','R','MAE','AUC','BS','tn', 'fp', 'fn', 'tp'],index=range(50))\n    rf=pd.DataFrame(columns=['P','ACC','R','MAE','AUC','BS'],index=range(50))\n    dmlp=pd.DataFrame(columns=['P','ACC','R','MAE','AUC','BS'],index=range(50))\n    dgbm=pd.DataFrame(columns=['P','ACC','R','MAE','AUC','BS'],index=range(50))\n    xgboost=pd.DataFrame(columns=['P','ACC','R','MAE','AUC','BS'],index=range(50))\n    dt_svm=pd.DataFrame(columns=['P','ACC','R','MAE','AUC','BS'],index=range(50))\n    dt_knn=pd.DataFrame(columns=['P','ACC','R','MAE','AUC','BS'],index=range(50))\n    dt_lg_regu=pd.DataFrame(columns=['P','ACC','R','MAE','AUC','BS'],index=range(50))\n    dt_lg=pd.DataFrame(columns=['P','ACC','R','MAE','AUC','BS'],index=range(50))\n                        \n    kf=RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=42)\n                        \n    \n    kf.get_n_splits(x,y)\n    k=0\n    for train_index, test_index in kf.split(x,y):\n\n\n        x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        \n        clf_dt=DecisionTree_classif(x_train,y_train,lista_param[0])\n        dt,lista1,p1,r1=calculate_metrics(x_train, x_test,y_train, y_test,dt,k,clf_dt,lista1,p1,r1)\n        \n        clf_rf=RandomForest_classif(x_train,y_train,lista_param[1])\n        rf,lista2,p2,r2=calculate_metrics(x_train, x_test,y_train, y_test,rf,k,clf_rf,lista2,p2,r2)\n        \n        clf_svm=svc_classif(x_train,y_train,lista_param[2])\n        dt_svm,lista3,p3,r3=calculate_metrics(x_train, x_test,y_train, y_test, dt_svm,k,clf_svm,lista3,p3,r3)\n       \n        clf_knn=Kneighbors_classif(x_train,y_train,lista_param[3])\n        dt_knn,lista4,p4,r4=calculate_metrics(x_train, x_test,y_train, y_test, dt_knn,k,clf_knn,lista4,p4,r4)\n        \n        clf_gbm=gb_classif(x_train,y_train,lista_param[4])\n        dgbm,lista5,p5,r5=calculate_metrics(x_train, x_test,y_train, y_test,dgbm,k,clf_gbm,lista5,p5,r5)\n        \n        clf_lg_regu=lg_with_regu_classif(x_train,y_train,lista_param[5])\n        dt_lg_regu,lista6,p6,r6=calculate_metrics(x_train, x_test,y_train, y_test,dt_lg_regu,k,\n                                                  clf_lg_regu,lista6,p6,r6)\n        \n        clf_lg=lg_without_regu_classif(x_train,y_train,lista_param[6])\n        dt_lg,lista7,p7,r7=calculate_metrics(x_train, x_test,y_train, y_test,dt_lg,k,clf_lg,lista7,p7,r7)\n        \n        clf_mlp=mpl_classif(x_train,y_train,lista_param[7])\n        dmlp,lista8,p8,r8=calculate_metrics(x_train, x_test,y_train, y_test,dmlp,k,clf_mlp,lista8,p8,r8)\n        \n        clf_xgboost=xgb_classif(x_train,y_train,lista_param[8])\n        xgboost,lista9,p9,r9=calculate_metrics(x_train, x_test,y_train, y_test,xgboost,k,clf_xgboost,lista9,p9,r9)\n        print(k)\n        k+=1\n        data=pd.DataFrame([dt.iloc[:,:6].mean(),rf.iloc[:,:6].mean(),dgbm.iloc[:,:6].mean(),xgboost.iloc[:,:6].mean(),dmlp.iloc[:,:6].mean(),dt_svm.iloc[:,:6].mean(),dt_knn.iloc[:,:6].mean(),dt_lg_regu.iloc[:,:6].mean(),dt_lg.iloc[:,:6].mean()])\n        listas=[lista1,lista2,lista3,lista4,lista5,lista6,lista7,lista8,lista9]\n        cof_matrix=[dt.iloc[:,6:],rf.iloc[:,6:],dgbm.iloc[:,6:],xgboost.iloc[:,6:],dmlp.iloc[:,6:],dt_svm.iloc[:,6:],dt_knn.iloc[:,6:],dt_lg_regu.iloc[:,6:],dt_lg.iloc[:,6:]]\n        p_list=[p1,p2,p3,p4,p5,p6,p7,p8,p9]\n        r_list=[r1,r2,r3,r4,r5,r6,r7,r8,r9]\n    return  data,listas,cof_matrix,p_list,r_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def majority_voting(x,y,estimator):\n    \n    dt=pd.DataFrame(columns=['P','ACC','R','MAE','AUC','BS'],index=range(50))\n    \n    kf=RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=42)\n                        \n    \n    kf.get_n_splits(x,y)\n    k=0\n    for train_index, test_index in kf.split(x,y):\n\n        x_train, x_test = x.iloc[train_index], x.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        \n        # Voting Classifier with hard voting \n        vot_hard = VotingClassifier(estimators = estimator, voting ='hard') \n        vot_hard=vot_hard.fit(x_train, y_train) \n        \n        dt= calculate_metrics_voting(x_train, x_test,y_train, y_test, dt,k,vot_hard)\n        k+=1\n    return dt.mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc_curve(listas,dt,name):\n    f=[np.linspace(0, 1, 100),np.linspace(0, 1, 100),np.linspace(0, 1, 100),np.linspace(0, 1, 100),\n       np.linspace(0, 1, 100),np.linspace(0, 1, 100),\n       np.linspace(0, 1, 100),np.linspace(0, 1, 100),np.linspace(0, 1, 100)]\n    t=[np.mean(listas[0], axis=0),np.mean(listas[1], axis=0),np.mean(listas[2], axis=0),np.mean(listas[3],\n        axis=0),np.mean(listas[4], axis=0),np.mean(listas[5], axis=0),np.mean(listas[6], axis=0),np.mean(listas[7], axis=0),np.mean(listas[8], axis=0)]\n   \n    aucs = [metrics.auc(f[0], t[0]),metrics.auc(f[1], t[1]),metrics.auc(f[2], t[2]),\n               metrics.auc(f[3], t[3]),metrics.auc(f[4], t[4]),metrics.auc(f[5], t[5]),\n               metrics.auc(f[6], t[6]),metrics.auc(f[7], t[7]),metrics.auc(f[8], t[8])]\n    mean_tpr = np.mean(t, axis=0)\n    mean_fpr=np.linspace(0, 1, 100)\n    mean_tpr[-1] = 1.0\n    mean_auc = metrics.auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n   \n    \n    fig = plt.figure(figsize=(8,6))\n    for k in range(9):\n        plt.plot(f[k],t[k], \n            label=\"{}, AUC={:.2f}\".format(dt.index[k],float(aucs[k])))\n\n    plt.plot([0,1], [0,1], color='red', linestyle='--',label='Chance')\n    plt.plot(mean_fpr, mean_tpr, color='b',linestyle='--',\n             label='Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n        lw=2, alpha=.8)\n\n    std_tpr = np.std(t, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n                    label=r'$\\pm$ 1 std. dev.')\n    plt.xticks(np.arange(0.0, 1.1, step=0.1))\n    plt.xlabel(\"False Positive Rate(Positive label:0)\", fontsize=15)\n\n    plt.yticks(np.arange(0.0, 1.1, step=0.1))\n    plt.ylabel(\"True Positive Rate(Positive label:0)\", fontsize=15)\n\n    \n    plt.legend(prop={'size':13}, loc='lower right')\n    \n    plt.savefig(name)\n#     plt.show()\n    \ndef plot_pr_curve( y_real,y_proba,dt,name):\n    fig = plt.figure(figsize=(8,6))\n    mean_y_real=[]\n    mean_y_prob=[]\n    aucs=[]\n    for k in range(9):\n        real = np.concatenate(y_real[k])\n        proba = np.concatenate(y_proba[k])\n        mean_y_real.append(real)\n        mean_y_prob.append(proba)\n        precision, recall, _ = precision_recall_curve(real, proba)\n        y_auc=metrics.average_precision_score(real,proba)\n        aucs.append(y_auc)\n        plt.plot(recall,precision,\n        label=\"{}, AP={:.2f}\".format(dt.index[k],y_auc))\n\n    mean_y_real = np.concatenate(mean_y_real)\n    mean_y_prob = np.concatenate(mean_y_prob)\n    mprecision, mrecall, _ = precision_recall_curve(mean_y_real, mean_y_prob)\n    std_auc = np.std(aucs)\n    mean_auc=metrics.average_precision_score(mean_y_real,mean_y_prob)\n    plt.plot(mrecall,mprecision,color='b',linestyle='--',\n        label='Mean PR (AP = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n        lw=2, alpha=.8)\n    \n    plt.xlabel('Recall(Positive label:0)',fontsize=15)\n    plt.ylabel('Precision(Positive label:0)',fontsize=15)\n    plt.legend(loc=\"lower right\")\n    plt.savefig(name)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Rapid test!","metadata":{}},{"cell_type":"markdown","source":"balanced","metadata":{}},{"cell_type":"code","source":"\nlista_param=[\n             {},\n             {},\n             {'kernel': 'rbf', 'C': 6,'probability':True},\n             {'weights': 'distance', 'n_neighbors': 15,'n_jobs': None, 'leaf_size': 5, 'algorithm': 'ball_tree'},\n             {'n_estimators':500,'max_depth':5},\n             {'solver': 'liblinear', 'penalty': 'l2','C': 0.1},\n             {'solver': 'liblinear', 'penalty': 'l2','C': 1.0},\n             {'solver': 'adam', 'random_state': 1, 'max_iter': 1200, 'learning_rate': 'adaptive','alpha': 0.0001, 'activation': 'relu'},\n             {'n_estimators':300,'max_depth':9,'min_child_weight': 2}]\n\nd_rapid,listas,cof_matrix,p_list,r_list=k_fold_cross_validation(data_rapid.iloc[:,0:13],data_rapid['Class'],lista_param) \nd_rapid=d_rapid.rename(index={0:'DT',1:'RF',2:'GBM', 3:'XGBoost',4:'Mlp',5:'SVM',6:'KNN',7:'LRR',8:'LRNR'})\nd_rapid.astype(float) \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(listas,d_rapid,'rapid_balanced_test_roc_curve')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pr_curve( p_list,r_list,d_rapid,'rapid_balanced_test_pr_curve')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"removing columns","metadata":{}},{"cell_type":"code","source":"\nd_rapid_rem,listas,c,p,r=k_fold_cross_validation(data_rapid_rem.iloc[:,0:9],data_rapid_rem['Class'],lista_param) \nd_rapid_rem=d_rapid_rem.rename(index={0:'DT',1:'RF',2:'GBM', 3:'XGBoost',4:'Mlp',5:'SVM',6:'KNN',7:'LRR',8:'LRNR'})\nd_rapid_rem.astype(float) \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"majority voting rapid balanced test","metadata":{}},{"cell_type":"code","source":"#GBM, DT, RF, and XGBoost\n# group / ensemble of models \n\nestimator = [] \nestimator.append(('RF',RandomForestClassifier()))\nestimator.append(('DT',tree.DecisionTreeClassifier()))\nestimator.append(('GBM',GradientBoostingClassifier(n_estimators= 500,max_depth= 5)))\nestimator.append(('XGBoost',xgb.XGBClassifier(n_estimators=300,max_depth=9,min_child_weight= 2)))\nmajority_voting(data_rapid.iloc[:,0:10],data_rapid['Class'],estimator)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MLP, SVM, KNN, LRR, and LRNR\n\n# group / ensemble of models \n\nestimator = [] \nestimator.append(('SVM',svm.SVC(kernel='rbf', C= 6)))\nestimator.append(('KNN',KNeighborsClassifier(weights= 'distance',n_neighbors=15,n_jobs= None,leaf_size= 5, algorithm= 'ball_tree')))\nestimator.append(('LRR',LogisticRegression(solver= 'liblinear',penalty= 'l2',C= 0.1)))\nestimator.append(('LRNR',LogisticRegression(solver= 'liblinear', penalty= 'l2',C=1.0)))\nestimator.append(('MLP',MLPClassifier(solver= 'adam',random_state= 1, max_iter= 1200,learning_rate= 'adaptive',alpha= 0.0001,activation='relu')))\nmajority_voting(data_rapid.iloc[:,0:13],data_rapid['Class'],estimator)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"unbalanced","metadata":{}},{"cell_type":"code","source":"\nlista_param=[\n{},\n{},\n{'kernel': 'rbf', 'C': 11,'probability':True},\n{'weights': 'distance', 'n_neighbors': 5, 'n_jobs': None, 'leaf_size': 3, 'algorithm': 'kd_tree'},\n{'n_estimators':500,'max_depth':5},\n{'solver': 'saga', 'penalty': 'l2', 'max_iter': 1200, 'C': 0.1},\n{'solver': 'lbfgs', 'penalty': 'l2', 'max_iter': 400, 'C': 4.0},\n{'max_iter': 1800,'solver':'lbfgs', 'alpha':1e-5, 'random_state':42},\n{'n_estimators':300,'max_depth':9,'min_child_weight': 2}]            \n\n\nd_rapid_unb,rlistas,rcof_matrix,rp_list,rr_list=k_fold_cross_validation(data_rapid_unb.iloc[:,0:13],data_rapid_unb['Class'],lista_param) \nd_rapid_unb=d_rapid_unb.rename(index={0:'Decision Tree',1:'Random Forest',2:'GBM', 3:'XGBoost',4:'Mlp',\n                         5:'SVM',6:'KNN',7:'LRR',8:'LRNR'})\nd_rapid_unb=d_rapid_unb.astype(float) \nd_rapid_unb\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(rlistas,d_rapid_unb,'rapid_unbalanced_test_roc_curve')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pr_curve( rp_list,rr_list,d_rapid_unb,'rapid_unbalanced_test_pr_curve')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"removing columns","metadata":{}},{"cell_type":"code","source":"\nd_rapid_unb_rem,listas,c,p,r=k_fold_cross_validation(data_rapid_unb_rem.iloc[:,0:9],data_rapid_unb_rem['Class'],lista_param) \nd_rapid_unb_rem=d_rapid_unb_rem.rename(index={0:'Decision Tree',1:'Random Forest',2:'GBM', 3:'XGBoost',4:'Mlp',\n                         5:'SVM',6:'KNN',7:'LRR',8:'LRNR'})\nd_rapid_unb_rem.astype(float) \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"majoraty voting","metadata":{}},{"cell_type":"code","source":"#GBM, DT, RF, and XGBoost\n# group / ensemble of models \n\nestimator = [] \nestimator.append(('RF',RandomForestClassifier()))\nestimator.append(('DT',tree.DecisionTreeClassifier()))\nestimator.append(('GBM',GradientBoostingClassifier(n_estimators= 500,max_depth= 5)))\nestimator.append(('XGBoost',xgb.XGBClassifier(n_estimators=300,max_depth=9,min_child_weight= 2)))\nmajority_voting(data_rapid_unb.iloc[:,0:13],data_rapid_unb['Class'],estimator)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MLP, SVM, KNN, LRR, and LRNR\n\n# group / ensemble of models \n\nestimator = [] \nestimator.append(('SVM',svm.SVC(kernel='rbf', C= 11)))\nestimator.append(('KNN',KNeighborsClassifier(weights= 'distance',n_neighbors=5,n_jobs= None,leaf_size= 3, algorithm= 'ball_tree')))\nestimator.append(('LRR',LogisticRegression(solver ='saga',penalty ='l2',max_iter =1200,C= 0.1)))\nestimator.append(('LRNR',LogisticRegression(solver= 'lbfgs',penalty= 'l2',max_iter= 400,C=4.0)))\nestimator.append(('MLP',MLPClassifier(max_iter= 1800,solver='lbfgs',alpha=1e-5,random_state=42)))\nmajority_voting(data_rapid_unb.iloc[:,0:13],data_rapid_unb['Class'],estimator)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test pcr prediction!","metadata":{}},{"cell_type":"markdown","source":"balanced","metadata":{}},{"cell_type":"code","source":"lista_param=[\n{},\n{},\n{'kernel': 'rbf', 'C': 7,'probability':True},\n{'weights': 'distance', 'n_neighbors': 5, 'n_jobs': None, 'leaf_size': 3, 'algorithm': 'kd_tree'},\n{'n_estimators':500,'max_depth':5},\n{'solver': 'liblinear', 'penalty': 'l2','C': 0.1},\n{'solver': 'liblinear', 'penalty': 'l2','C': 1.0},\n{'max_iter': 1800,'solver':'lbfgs', 'alpha':1e-5, 'random_state':42},\n{'n_estimators':300,'max_depth':9,'min_child_weight': 2}]\n\nd_pcr,listas,cof_matrix,p_list,r_list=k_fold_cross_validation(data_pcr.iloc[:,0:13],data_pcr['Class'],lista_param)  \nd_pcr=d_pcr.rename(index={0:'Decision Tree',1:'Random Forest',2:'GBM', 3:'XGBoost',4:'Mlp',\n                         5:'SVM',6:'KNN',7:'LRR',8:'LRNR'})\nd_pcr=d_pcr.astype(float) \nd_pcr\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplot_roc_curve(listas,d_pcr,'pcr_balanced_test_roc_curve')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pr_curve(p_list,r_list,d_pcr,'pcr_balanced_test_pr_curve')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"removing columns","metadata":{}},{"cell_type":"code","source":"\nd_pcr_rem,listas,c,p,r=k_fold_cross_validation(data_pcr_rem.iloc[:,0:9],data_pcr_rem['Class'],lista_param)  \nd_pcr_rem=d_pcr_rem.rename(index={0:'Decision Tree',1:'Random Forest',2:'GBM', 3:'XGBoost',4:'Mlp',\n                         5:'SVM',6:'KNN',7:'LRR',8:'LRNR'})\nd_pcr_rem.astype(float) \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"majority voting","metadata":{}},{"cell_type":"code","source":"GBM, DT, RF, and XGBoost\ngroup / ensemble of models \n\nestimator = [] \nestimator.append(('RF',RandomForestClassifier()))\nestimator.append(('DT',tree.DecisionTreeClassifier()))\nestimator.append(('GBM',GradientBoostingClassifier(n_estimators= 500,max_depth= 5)))\nestimator.append(('XGBoost',xgb.XGBClassifier(n_estimators=300,max_depth=9,min_child_weight= 2)))\nmajority_voting(data_pcr.iloc[:,0:13],data_pcr['Class'],estimator)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MLP, SVM, KNN, LRR, and LRNR\n\ngroup / ensemble of models \n\nestimator = [] \nestimator.append(('SVM',svm.SVC(kernel='rbf', C= 7)))\nestimator.append(('KNN',KNeighborsClassifier(weights= 'distance',n_neighbors=5,n_jobs= None,leaf_size= 3, algorithm= 'kd_tree')))\nestimator.append(('LRR',LogisticRegression(solver= 'liblinear',penalty= 'l2',C= 0.1)))\nestimator.append(('LRNR',LogisticRegression(solver='liblinear',penalty= 'l2',C=1.0)))\nestimator.append(('MLP',MLPClassifier(max_iter= 1800,solver='lbfgs',alpha=1e-5,random_state=42)))\nmajority_voting(data_pcr.iloc[:,0:13],data_pcr['Class'],estimator)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"unbalanced","metadata":{}},{"cell_type":"code","source":"lista_param=[\n{},\n{},\n{'kernel': 'rbf', 'C': 6,'probability':True},\n{'weights': 'distance', 'n_neighbors': 8, 'n_jobs': -1, 'leaf_size': 2, 'algorithm': 'auto'},\n{'n_estimators':500,'max_depth':5},\n{'solver': 'liblinear', 'penalty': 'l2','C': 0.1},\n{'solver': 'liblinear', 'penalty': 'l2','C': 1.0},\n{'max_iter': 1800,'solver':'lbfgs', 'alpha':1e-5, 'random_state':42},\n{'n_estimators':300,'max_depth':9,'min_child_weight': 2}]\n\nd_pcr_unb,punblistas,punbcof_matrix,punbp_list,punbr_list=k_fold_cross_validation(data_pcr_unb.iloc[:,0:13],data_pcr_unb['Class'],lista_param)  \nd_pcr_unb=d_pcr_unb.rename(index={0:'Decision Tree',1:'Random Forest',2:'GBM', 3:'XGBoost',4:'Mlp',\n                         5:'SVM',6:'KNN',7:'LRR',8:'LRNR'})\nd_pcr_unb=d_pcr_unb.astype(float) \nd_pcr_unb\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(punblistas,d_pcr_unb,'pcr_unbalanced_test_roc_curve')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pr_curve(punbp_list,punbr_list,d_pcr_unb,'pcr_unbalanced_test_pr_curve')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"removing columns","metadata":{}},{"cell_type":"code","source":"\nd_pcr_unb_rem,listas,c,p,r=k_fold_cross_validation(data_pcr_unb_rem.iloc[:,0:9],data_pcr_unb_rem['Class'],lista_param)  \nd_pcr_unb_rem=d_pcr_unb_rem.rename(index={0:'Decision Tree',1:'Random Forest',2:'GBM', 3:'XGBoost',4:'Mlp',\n                         5:'SVM',6:'KNN',7:'LRR',8:'LRNR'})\nd_pcr_unb_rem.astype(float) \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"majority voting","metadata":{}},{"cell_type":"code","source":"#GBM, DT, RF, and XGBoost\n# group / ensemble of models \n\nestimator = [] \nestimator.append(('RF',RandomForestClassifier()))\nestimator.append(('DT',tree.DecisionTreeClassifier()))\nestimator.append(('GBM',GradientBoostingClassifier(n_estimators= 500,max_depth= 5)))\nestimator.append(('XGBoost',xgb.XGBClassifier(n_estimators=300,max_depth=9,min_child_weight= 2)))\nmajority_voting(data_pcr_unb.iloc[:,0:13],data_pcr_unb['Class'],estimator)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MLP, SVM, KNN, LRR, and LRNR\n\n# group / ensemble of models \n\nestimator = [] \nestimator.append(('SVM',svm.SVC(kernel='rbf', C= 6)))\nestimator.append(('KNN',KNeighborsClassifier(weights='distance',n_neighbors= 8,n_jobs= -1,leaf_size= 2,algorithm= 'auto')))\nestimator.append(('LRR',LogisticRegression(solver= 'liblinear',penalty= 'l2',C= 0.1)))\nestimator.append(('LRNR',LogisticRegression(solver= 'liblinear',penalty= 'l2',C=1.0)))\nestimator.append(('MLP',MLPClassifier(max_iter=1800,solver='lbfgs',alpha=1e-5,random_state=42)))\nmajority_voting(data_pcr_unb.iloc[:,0:13],data_pcr_unb['Class'],estimator)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Both test prediction!","metadata":{}},{"cell_type":"markdown","source":"balanced","metadata":{}},{"cell_type":"code","source":"lista_param=[{},{},{'kernel': 'rbf', 'C': 9,'probability':True},\n             {'weights': 'distance','n_neighbors': 8, 'n_jobs': -1,'leaf_size': 5, 'algorithm': 'brute'},\n             {'n_estimators':500,'max_depth':5},\n             {'solver': 'liblinear', 'penalty': 'l2','C': 0.1},\n             {'solver': 'liblinear', 'penalty': 'l2','C': 1.0},\n             {'max_iter': 1200,'solver':'lbfgs', 'alpha':1e-5, 'random_state':42},\n             {'n_estimators':300,'max_depth':9,'min_child_weight': 2}]\n\n\nd,dlistas,dcof_matrix,dp_list,dr_list=k_fold_cross_validation(data_both.iloc[:,0:13],data_both['Class'],lista_param)\n  \nd=d.rename(index={0:'Decision Tree',1:'Random Forest',2:'GBM', 3:'XGBoost',4:'Mlp',\n                         5:'SVM',6:'KNN',7:'LRR',8:'LRNR'})\nd=d.astype(float) \nd\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(dlistas,d,'both_balanced_tests_roc_curve')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pr_curve(dp_list,dr_list,d,'both_balanced_test_pr_curve')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"removing columns","metadata":{}},{"cell_type":"code","source":"\nd_rem,listas,c,p,r=k_fold_cross_validation(data_both_rem.iloc[:,0:9],data_both_rem['Class'],lista_param)\n  \nd_rem=d_rem.rename(index={0:'Decision Tree',1:'Random Forest',2:'GBM', 3:'XGBoost',4:'Mlp',\n                         5:'SVM',6:'KNN',7:'LRR',8:'LRNR'})\nd_rem.astype(float) \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"majority voting","metadata":{}},{"cell_type":"code","source":"GBM, DT, RF, and XGBoost\ngroup / ensemble of models \n\nestimator = [] \nestimator.append(('RF',RandomForestClassifier()))\nestimator.append(('DT',tree.DecisionTreeClassifier()))\nestimator.append(('GBM',GradientBoostingClassifier(n_estimators= 500,max_depth= 5)))\nestimator.append(('XGBoost',xgb.XGBClassifier(n_estimators=300,max_depth=9,min_child_weight= 2)))\nmajority_voting(data_both.iloc[:,0:13],data_both['Class'],estimator)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MLP, SVM, KNN, LRR, and LRNR\n\n# group / ensemble of models \n\nestimator = [] \nestimator.append(('SVM',svm.SVC(kernel='rbf', C= 9)))\nestimator.append(('KNN',KNeighborsClassifier(weights= 'distance',n_neighbors=8,n_jobs= -1,leaf_size= 5,algorithm= 'brute')))\nestimator.append(('LRR',LogisticRegression(solver= 'liblinear',penalty= 'l2',C= 0.1)))\nestimator.append(('LRNR',LogisticRegression(solver= 'liblinear',penalty= 'l2',C=1.0)))\nestimator.append(('MLP',MLPClassifier(max_iter=1200,solver='lbfgs',alpha=1e-5,random_state=42)))\nmajority_voting(data_both.iloc[:,0:13],data_both['Class'],estimator)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"unbalanced","metadata":{}},{"cell_type":"code","source":"lista_param=[{},{},{'kernel': 'rbf', 'C': 8,'probability':True},\n             {'n_neighbors': 15},\n             {'n_estimators':500,'max_depth':5},\n             {'solver': 'liblinear', 'penalty': 'l2','C': 0.1},\n             {'solver': 'liblinear', 'penalty': 'l2','C': 1.0},\n             {'max_iter': 1800,'solver':'lbfgs', 'alpha':1e-5, 'random_state':42},\n            {'n_estimators':300,'max_depth':9,'min_child_weight': 2}]\nboth_data_unb = data_both_unb.copy()\nd2,d2listas,d2cof_matrix,d2p_list,d2r_list=k_fold_cross_validation(both_data_unb.iloc[:,0:13],both_data_unb['Class'],lista_param)\n  \nd2=d2.rename(index={0:'Decision Tree',1:'Random Forest',2:'GBM', 3:'XGBoost',4:'Mlp',\n                         5:'SVM',6:'KNN',7:'LRR',8:'LRNR'})\nd2=d2.astype(float) \nd2\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc_curve(d2listas,d2,'both_unbalanced_tests_roc_curve')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_pr_curve(d2p_list,d2r_list,d2,'both_unbalanced_test_pr_curve')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"removing columns","metadata":{}},{"cell_type":"code","source":"\nd2_rem,listas,c,p,r=k_fold_cross_validation(data_both_unb_rem.iloc[:,0:9],data_both_unb_rem['Class'],lista_param)\n  \nd2_rem=d2_rem.rename(index={0:'Decision Tree',1:'Random Forest',2:'GBM', 3:'XGBoost',4:'Mlp',\n                         5:'SVM',6:'KNN',7:'LRR',8:'LRNR'})\nd2_rem.astype(float)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"majority voting","metadata":{}},{"cell_type":"code","source":"#GBM, DT, RF, and XGBoost\n# group / ensemble of models \n\nestimator = [] \nestimator.append(('RF',RandomForestClassifier()))\nestimator.append(('DT',tree.DecisionTreeClassifier()))\nestimator.append(('GBM',GradientBoostingClassifier(n_estimators= 500,max_depth= 5)))\nestimator.append(('XGBoost',xgb.XGBClassifier(n_estimators=300,max_depth=9,min_child_weight= 2)))\nmajority_voting(both_data_unb.iloc[:,0:13],both_data_unb['Class'],estimator)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#MLP, SVM, KNN, LRR, and LRNR\n\n# group / ensemble of models \n\nestimator = [] \nestimator.append(('SVM',svm.SVC(kernel='rbf', C= 8)))\nestimator.append(('KNN',KNeighborsClassifier(n_neighbors=15)))\nestimator.append(('LRR',LogisticRegression(solver= 'liblinear',penalty= 'l2',C= 0.1)))\nestimator.append(('LRNR',LogisticRegression(solver= 'liblinear',penalty= 'l2',C=1.0)))\nestimator.append(('MLP',MLPClassifier(max_iter=1800,solver='lbfgs',alpha=1e-5,random_state=42)))\nmajority_voting(both_data_unb.iloc[:,0:13],both_data_unb['Class'],estimator)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}